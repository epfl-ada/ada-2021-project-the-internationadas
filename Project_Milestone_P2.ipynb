{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kopie von Copy of Copia di Tutorial_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vyO72L0J9PP"
      },
      "source": [
        "### **Do people with different ideologies speak differently?**\n",
        "ADA Project Milestone P2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA_fe4oxWT6p"
      },
      "source": [
        "# Mouting the Google Drive\n",
        "\n",
        "It is possible to mount your Google Drive to Colab if you need additional storage or if you need to use files from it. To do that run (click on play button or use keyboard shortcut 'Command/Ctrl+Enter') the following code cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEcIlRwfWY4C"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Nfr-aXhWbFk"
      },
      "source": [
        " \n",
        "\n",
        "1.   After running the cell, URL will appear.\n",
        "\n",
        "2.   Following this URL, you will be redirected to the page where you need to choose Google Drive account to mount to.\n",
        "\n",
        "3.   You will further be asked to give Google Drive Stream a permission to access the chosen Google account\n",
        "\n",
        "4.   After granting the access, authorization code will be given to you\n",
        "\n",
        "5.   Copy the authorization code into the dedicated textbox in Colab under '*Enter your authorization code:*' writing\n",
        "\n",
        "After copying the authorization code, you should get the message saying '*Mounted at /content/gdrive*'\n",
        "\n",
        "Path to the files from the mounted Drive will then be '/content/drive/MyDrive/'. By opening the Files tab (left sidebar, folder icon) you should also be able to see the accessible files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLRO9Z9RUguz"
      },
      "source": [
        "# Cleaning of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYboyMBwhBL1"
      },
      "source": [
        "!pip install pandas==1.0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0M9XTehUfcA"
      },
      "source": [
        "# Imports\n",
        "import bz2\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFOKf71SU36O"
      },
      "source": [
        "## Load/filter/merge for all years quotebank and wikidata samples<br>\n",
        "Definition of functions used during data cleaning and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmWhmdfpOeoz"
      },
      "source": [
        "# Load and filter wikidata for our purpose\n",
        "def load_filter_wikidata_df(path_to_file):\n",
        "  # load from file\n",
        "  columns = [\"id\", \"gender\", \"occupation\", \"party\"]\n",
        "  df_wikidata = pd.read_parquet(path_to_file, columns=columns)\n",
        "\n",
        "  # Filter\n",
        "  # Remove rows without a party\n",
        "  df_wikidata_parties = df_wikidata.dropna(subset=['party'])\n",
        "\n",
        "  # Select only rows with either republican or democrats party\n",
        "  QID_republicans = \"Q29468\"\n",
        "  QID_democrats = \"Q29552\"\n",
        "  df_wikidata_filtered = df_wikidata_parties[df_wikidata_parties.apply(lambda x: (QID_republicans in x['party']) or (QID_democrats in x['party']) , axis=1)]\n",
        "\n",
        "  return df_wikidata_filtered"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o93OsnXfUu58"
      },
      "source": [
        "# Perform all operations needed on quotebank dataset (load/filter/merge with wikidata/store)\n",
        "def handle_quotebank_df(input_file, output_file, chunk_size, df_wikidata, n_chunks=0):\n",
        "  curr_chunk = 0\n",
        "  chunk_list = []\n",
        "\n",
        "  # read input file by chunks (as the whole file can't fit into memory)\n",
        "  reader = pd.read_json(input_file, lines=True, compression='bz2', chunksize=chunk_size)\n",
        "  for chunk in reader:\n",
        "    #if curr_chunk == n_chunks:\n",
        "     # break\n",
        "    curr_chunk += 1\n",
        "    # append only when the speaker is knows (the best % is not from \"None\" speaker)\n",
        "    filtered_chunk = chunk[chunk['speaker'] != 'None' ][['quoteID', 'quotation','speaker', 'qids', 'probas']]\n",
        "    \n",
        "    # apply filter to single chunk\n",
        "    filtered_chunk = filter_quotebank_df(filtered_chunk)\n",
        "\n",
        "    # merge single chunk with wikidata in order to reduce even further the dataset and allow RAM to store it\n",
        "    filtered_merged_chunk = merge_quotebank_wikidata_df(filtered_chunk, df_wikidata)\n",
        "    \n",
        "    # append to output file a single chunk\n",
        "    filtered_merged_chunk.to_csv(output_file, mode='a', compression='bz2')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d116TpH2NYEa"
      },
      "source": [
        "# Filter unuset entries in wikidata\n",
        "def filter_quotebank_df(df):\n",
        "  # remove the data with not unique qid speaker because we are not sure who is the speaker: speakers with same name but different qids\n",
        "  df_filtered = df[df.apply(lambda x: len(x['qids']) == 1, axis=1)]\n",
        "\n",
        "  # now we don't have anymore list of quids (only 1 quid per entry possible), so remove list and store only the single value\n",
        "  df_filtered['qids'] = df_filtered['qids'].apply(lambda x: x[0])\n",
        "\n",
        "  return df_filtered"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnfAScdVNX6i"
      },
      "source": [
        "# Merge quotebank and wikidata entries on QID\n",
        "def merge_quotebank_wikidata_df(df_quotebank, df_wikidata):\n",
        "  #merge quotebank data with wikidata \n",
        "  df_merged = df_quotebank.merge(right=df_wikidata, how='inner', left_on='qids', right_on='id')\n",
        "\n",
        "  #drop the id column because we already have the qid\n",
        "  df_merged = df_merged.drop(labels='id', axis=1)\n",
        "  \n",
        "  return df_merged"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NovbYnVqNXyh"
      },
      "source": [
        "# Append to output file a single chunk\n",
        "def store_chunk_df(path_to_file, df):\n",
        "  # Dump the single chunk to csv, appending it to previously written chunks\n",
        "  filtered_merged_chunk.to_csv(output_file, mode='a', compression='bz2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajttCNeyt0lk"
      },
      "source": [
        "Actual data cleaning and preprocessing is done here. The final dataframe for each year is saved in an additional .bz2 file.<br>\n",
        "*(Note that it takes around 5 hours to run the following cell)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ95GHaNLnbq"
      },
      "source": [
        "chunk_size = 100000\n",
        "# n_chunks = 10\n",
        "\n",
        "path_to_parquet = '/content/drive/MyDrive/Project datasets/speaker_attributes.parquet'\n",
        "df_wikidata = load_filter_wikidata_df(path_to_parquet)\n",
        "\n",
        "input_file = '/content/drive/MyDrive/Quotebank/quotes-2020.json.bz2'\n",
        "output_file = '/content/drive/MyDrive/Quotebank_Repub_Dem/quotes-2020-repub-dem.json.bz2'\n",
        "handle_quotebank_df(input_file, output_file, chunk_size, df_wikidata)\n",
        "\n",
        "input_file = '/content/drive/MyDrive/Quotebank/quotes-2019.json.bz2'\n",
        "output_file = '/content/drive/MyDrive/Quotebank_Repub_Dem/quotes-2019-repub-dem.json.bz2'\n",
        "handle_quotebank_df(input_file, output_file, chunk_size, df_wikidata)\n",
        "\n",
        "input_file = '/content/drive/MyDrive/Quotebank/quotes-2018.json.bz2'\n",
        "output_file = '/content/drive/MyDrive/Quotebank_Repub_Dem/quotes-2018-repub-dem.json.bz2'\n",
        "handle_quotebank_df(input_file, output_file, chunk_size, df_wikidata)\n",
        "\n",
        "input_file = '/content/drive/MyDrive/Quotebank/quotes-2017.json.bz2'\n",
        "output_file = '/content/drive/MyDrive/Quotebank_Repub_Dem/quotes-2017-repub-dem.json.bz2'\n",
        "handle_quotebank_df(input_file, output_file, chunk_size, df_wikidata)\n",
        "\n",
        "input_file = '/content/drive/MyDrive/Quotebank/quotes-2016.json.bz2'\n",
        "output_file = '/content/drive/MyDrive/Quotebank_Repub_Dem/quotes-2016-repub-dem.json.bz2'\n",
        "handle_quotebank_df(input_file, output_file, chunk_size, df_wikidata)\n",
        "\n",
        "input_file = '/content/drive/MyDrive/Quotebank/quotes-2015.json.bz2'\n",
        "output_file = '/content/drive/MyDrive/Quotebank_Repub_Dem/quotes-2015-repub-dem.json.bz2'\n",
        "handle_quotebank_df(input_file, output_file, chunk_size, df_wikidata)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHh7JmElv1e6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H17i-gzv37Z"
      },
      "source": [
        "## Load and merge preprocessed data for all years (2015-2020)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsvAWXIGwCcm"
      },
      "source": [
        "file_path_2020 = '/content/drive/MyDrive/Quotebank_Repub_Dem/quotes-2020-repub-dem.json.bz2'\n",
        "file_path_2019 = '/content/drive/MyDrive/Quotebank_Repub_Dem/quotes-2019-repub-dem.json.bz2'\n",
        "file_path_2018 = '/content/drive/MyDrive/Quotebank_Repub_Dem/quotes-2018-repub-dem.json.bz2'\n",
        "file_path_2017 = '/content/drive/MyDrive/Quotebank_Repub_Dem/quotes-2017-repub-dem.json.bz2'\n",
        "file_path_2016 = '/content/drive/MyDrive/Quotebank_Repub_Dem/quotes-2016-repub-dem.json.bz2'\n",
        "file_path_2015 = '/content/drive/MyDrive/Quotebank_Repub_Dem/quotes-2015-repub-dem.json.bz2'\n",
        "\n",
        "def load_and_merge():\n",
        "  df = pd.read_csv(file_path_2020, compression='bz2')\n",
        "  df_1 = pd.read_csv(file_path_2019, compression='bz2')\n",
        "  df = pd.concat([df, df_1], ignore_index=True)\n",
        "  print(\"Done\")\n",
        "  df_1 = pd.read_csv(file_path_2018, compression='bz2')\n",
        "  df = pd.concat([df, df_1], ignore_index=True)\n",
        "  print(\"Done\")\n",
        "  df_1 = pd.read_csv(file_path_2017, compression='bz2')\n",
        "  df = pd.concat([df, df_1], ignore_index=True)\n",
        "  print(\"Done\")\n",
        "  df_1 = pd.read_csv(file_path_2016, compression='bz2')\n",
        "  df = pd.concat([df, df_1], ignore_index=True)\n",
        "  print(\"Done\")\n",
        "  df_1 = pd.read_csv(file_path_2015, compression='bz2')\n",
        "  df = pd.concat([df, df_1], ignore_index=True)\n",
        "  print(\"Done\")\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y1cVosXxXtW"
      },
      "source": [
        "df = load_and_merge()\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVJepU7mQwVo"
      },
      "source": [
        "### OLD CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeyTn0mSQdsd"
      },
      "source": [
        "chunk_size = 100000\n",
        "n_chunks = 10\n",
        "input_file = '/content/drive/MyDrive/Quotebank/quotes-2020.json.bz2'\n",
        "\n",
        "def load_quotebank_df(path_to_file, chunk_size, n_chunks=0):\n",
        "  curr_chunk = 0\n",
        "  chunk_list = []\n",
        "  reader = pd.read_json(path_to_file, lines=True, compression='bz2', chunksize=chunk_size)\n",
        "  for chunk in reader:\n",
        "    #if curr_chunk == n_chunks:\n",
        "     # break\n",
        "    curr_chunk += 1\n",
        "    # append only when the speaker is knows (the best % is not from \"None\" speaker)\n",
        "    chunk_list.append(chunk[chunk['speaker'] != 'None' ][['quoteID', 'quotation','speaker', 'qids', 'probas']])\n",
        "  df = pd.concat(chunk_list)\n",
        "  print(curr_chunk)\n",
        "  return df\n",
        "\n",
        "df = load_quotebank_df(input_file, chunk_size, n_chunks)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CDKdnMEX30F"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUdds4F50vZT"
      },
      "source": [
        "check if there are duplicate quote IDs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaZCgCTVp5w_"
      },
      "source": [
        "(df['quoteID'].value_counts() > 1).any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d-SMNlr1-bz"
      },
      "source": [
        "check some conditions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H31wYz54gUOZ"
      },
      "source": [
        "remove the data with not unique qid speaker because we are not sure who is the speaker: speakers with same name but different qids\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxXAr46x2eMt"
      },
      "source": [
        "qid_array = df['qids'].values\n",
        "# number of qids per speaker\n",
        "n_qids = [len(list) for list in qid_array ]\n",
        "filtered = filter(lambda n: n > 1, n_qids)\n",
        "len(list(filtered))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fUAt2t7h4yS"
      },
      "source": [
        "df_filtered = df[df.apply(lambda x: len(x['qids']) == 1, axis=1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMRjR6r_jtqi"
      },
      "source": [
        "len(df_filtered)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mxZbIF8zCKu"
      },
      "source": [
        "# Since we removed the persons with more than on qid we can remove the list and leave only the value inside the list qids\n",
        "df_filtered['qids'] = df_filtered['qids'].apply(lambda x: x[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa8kDW-A0mVd"
      },
      "source": [
        "df_filtered.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5OkDS8AlNco"
      },
      "source": [
        "# Merge some attributes of the speakers from Wikidata "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amRjNm8ZjslQ"
      },
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTPMubglY4MW"
      },
      "source": [
        "path_to_parquet = '/content/drive/MyDrive/Project datasets/speaker_attributes.parquet'\n",
        "columns = [\"id\", \"gender\", \"occupation\", \"party\"]\n",
        "df_wikidata = pd.read_parquet(path_to_parquet, columns=columns)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viBNkKFGl3t2"
      },
      "source": [
        "df_wikidata"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLw_vQmomxbr"
      },
      "source": [
        "filter none partys"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD6rJoCxmwq2"
      },
      "source": [
        "df_wikidata_parties = df_wikidata.dropna(subset=['party'])\n",
        "df_wikidata_parties"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3z64eZDnNx-"
      },
      "source": [
        "Check if we have some rows with more one qid for the party"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfNH3vTorh9r"
      },
      "source": [
        "qid_array_party = df_wikidata_parties['party'].values\n",
        "# number of qids per speaker\n",
        "n_qids_party = [len(list) for list in qid_array_party ]\n",
        "filtered = filter(lambda n: n > 1, n_qids_party)\n",
        "len(list(filtered))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA4E6zJ9vhpt"
      },
      "source": [
        "filter the data and keep only the qid of republicans and the democrats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aLSNcuemac7"
      },
      "source": [
        "QID_republicans = \"Q29468\"\n",
        "QID_democrats = \"Q29552\"\n",
        "#df_wikidata_filtered = df_wikidata[(df_wikidata[\"party\"][:, 0] == QID_democrats) | (df_wikidata[\"party\"][:, 0] == QID_republicans)]\n",
        "df_wikidata_filtered = df_wikidata_parties[df_wikidata_parties.apply(lambda x: (QID_republicans in x['party']) or (QID_democrats in x['party']) , axis=1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f-upjIzoEQv"
      },
      "source": [
        "df_wikidata_filtered"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4fpCR3hv26k"
      },
      "source": [
        "check if we have more than party per person for the filtered data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjYQUHwZuGSK"
      },
      "source": [
        "qid_array_party = df_wikidata_filtered['party'].values\n",
        "# number of qids per speaker\n",
        "n_qids_party = [len(list) for list in qid_array_party ]\n",
        "filtered = filter(lambda n: n > 1, n_qids_party)\n",
        "len(list(filtered))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kMhQEmQvCdp"
      },
      "source": [
        "# Merge quotebank data with wikidata \n",
        "df_merged = df_filtered.merge(right=df_wikidata_filtered, how='inner', left_on='qids', right_on='id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ3Xjhgp1Ibn"
      },
      "source": [
        "# Drop the id column because we already have the qid\n",
        "df_merged = df_merged.drop(labels='id', axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1X51oOg2wWR"
      },
      "source": [
        "df_merged"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54VzfJVT3BCG"
      },
      "source": [
        "# Dump the new dataframe to json\n",
        "path_to_out = '/content/quotes-2020-repub-dem.json.bz2'\n",
        "df_merged.to_json(path_or_buf=path_to_out, compression='bz2')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRVJGCyR4a76"
      },
      "source": [
        "# Load the new file as a dataframe\n",
        "file_path = '/content/drive/MyDrive/Quotebank_Repub_Dem/quotes-2018-repub-dem.json.bz2'\n",
        "df = pd.read_csv(file_path, compression='bz2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_yuwLAQ5Mh0"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrQwGwY65Tm-"
      },
      "source": [
        "# Add number of quotations per qid to dataframe (Caution: this column is already aggregated)\n",
        "df['quotationCounts'] = df.groupby(['qids'])['quoteID'].transform(\"count\")\n",
        "\n",
        "# Visualize the number of quotations per speaker grouped by Democrats and Republicans\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 8))\n",
        "\n",
        "subplot = axes[0];\n",
        "subplot.hist(df[df['party'].str[0] == 'Q29468']['quotationCounts'].values, bins = 50, color=\"lightsteelblue\")\n",
        "subplot.set_title(\"Quotations per Speaker (Republican)\")\n",
        "subplot.set_xlabel(\"Quotation Count\")\n",
        "subplot.set_ylabel(\"Number of Speakers\")\n",
        "subplot.set_xlim([0,45000])\n",
        "subplot.set_ylim([0,80000])\n",
        "\n",
        "subplot = axes[1];\n",
        "subplot.hist(df[df['party'].str[0] == 'Q29552']['quotationCounts'].values, bins = 50, color=\"cornflowerblue\")\n",
        "subplot.set_title(\"Quotations per Speaker (Democrat)\")\n",
        "subplot.set_xlabel(\"Quotation Count\")\n",
        "subplot.set_ylabel(\"Number of Speakers\")\n",
        "subplot.set_xlim([0,45000])\n",
        "subplot.set_ylim([0,80000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6UoOTyshJOE"
      },
      "source": [
        "# Top Speakers of the Republican Party\n",
        "df[df['party'].str[0] == 'Q29468'].groupby(['qids']).mean().sort_values(by='quotationCounts', ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTFgmVPshTjk"
      },
      "source": [
        "# Top Speakers of the Democratic Party\n",
        "df[df['party'].str[0] == 'Q29552'].groupby(['qids']).mean().sort_values(by='quotationCounts', ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}